{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467ef9ac",
   "metadata": {},
   "source": [
    "# The second part of the assignment, IDS 2021-2022\n",
    "In this Jupyter notebook document all your results and the way you have obtained them. Please use the _Python environment_ provided for this part of the assignment. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing  other outputs you have generated that are not included in this notebook (such as pdf, jpg, and others). Please make sure that the other outputs are easily identifiable, i.e. use names as requested in the corresponding question. _You do not need to include the datasets._\n",
    "\n",
    "This is the _only_ submission that is required (Jupyter notebook + zip-file). A separate report is _not_ needed and will not be considered for grading. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "<font color=\"red\"> Please make sure to include the names and matriculation numbers of all group members in the slot provided below. </font> If a name or a student id is missing, the student will not receive any points.\n",
    "\n",
    "<font color=\"red\">Plan your time wisely. </font> A few parts of this assignment might take some time to run. It might be necessary to consider time management when you plan your group work.\n",
    "\n",
    "Hint: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119476e",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    1. Daniel Weißen (427 492)\n",
    "    \n",
    "    2. Felix Meyer (378 959)\n",
    "    \n",
    "    3. Lars Kreuzberg (368 924)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cabcf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "### Widgets\n",
    "#import ipywidgets as widgets\n",
    "\n",
    "### Data Handline\n",
    "import pandas as pd\n",
    "# avoid collapsed view for pandas dataframes\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np\n",
    "\n",
    "### Utility\n",
    "import math\n",
    "import string\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "### Plotting\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "# Matplotlib toolkits\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "# Gespatial data with cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "### Frequent Pattern Mining\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules as arule\n",
    "\n",
    "### Text Mining\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "### PM4Py\n",
    "import pm4py\n",
    "# Log Handling\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "# Statistics\n",
    "from pm4py.statistics.traces.generic.log import case_statistics\n",
    "# Filtering\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "# Discovery and Conformance Checking\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "# Visualization\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "\n",
    "pd.options.display.max_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6547aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas indexing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b3c8f",
   "metadata": {},
   "source": [
    "Be careful if you use the **matplotlib widget** magic. If you do not close the created plots, previous plots may change if you create a new one.\n",
    "So if you use this magic command, be careful about your outputs in your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4843d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d73d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q1 - Preprocessing (20 points)\n",
    "In this question, we consider a US-census dataset (**census_data.csv**).\n",
    "Each row contains statistics of a certain tract on a variety of, particularly income- and work-related, life aspects of US citizens.\n",
    "Short column description:\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| CensusTract | Tract |\n",
    "| State | State |\n",
    "| County | County |\n",
    "| TotalPop | Total population  |\n",
    "| Men | Number of men |\n",
    "| Women | Number of women |\n",
    "| Hispanic, White, Black, Native, Asian, Pacific | Percentage of ethnic group |\n",
    "| Citizen | Percentage of citizen |\n",
    "| Income | Median household income |\n",
    "| IncomePerCap | Income per capita |\n",
    "| Poverty | Poverty rate |\n",
    "| ChildPoverty | Child poverty rate |\n",
    "| Professional | Employed in management, business, science, and arts (percentage) |\n",
    "| Service, Office, Construction, Production | Other profession fields (percentage) |\n",
    "| PrivateWork, PublicWork | Employed in private / public sector (percentage) |\n",
    "| Drive, Carpool, Transit, Walk, OtherTransp | Means of commuting (percentage) |\n",
    "| WorkAtHome | Working at home (percentage) |\n",
    "| MeanCommute | Mean time for commuting |\n",
    "| Employed | Number of employed |\n",
    "| SelfEmployed | Self-employed (percentage) |\n",
    "| FamilyWork | Unpaid family work (percentage) |\n",
    "| Unemployment | Unemployment rate |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00d1de",
   "metadata": {},
   "source": [
    "## Loading the Data and Initial Quality Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eba3a0",
   "metadata": {},
   "source": [
    "**a)** Load the dataset into a dataframe `df`. <font color='red'>Use the CensusTract as index for your dataframe</font>. In doing so, ensure that the index is valid, that is, it does not contain any duplicate entries.\n",
    "\n",
    "**In the subsequent questions, only modify the dataframe `df` if explicitly requested. However, you can always create working copies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffc975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/census_data.csv', index_col='CensusTract')\n",
    "# I don't know if it is necessary to explicitely check if no index values appear twice, if yes use:\n",
    "# index = list(df.index)\n",
    "# len(index) != len(set(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320067e2",
   "metadata": {},
   "source": [
    "**b)** Show the data types of the dataframe columns as well as the first few rows. On the first sight, are there any data type problems (e.g., numerical columns having a non-numerical data type)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10ed82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6126f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279e874a",
   "metadata": {},
   "source": [
    "The column 'Employed' shows total number of people being employed whereas 'Unemployment' shows the percentage of unemployed people. Also the columns 'Men' and 'Women' are given as floating point numbers even though they are a total count and only integers make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f29bbf",
   "metadata": {},
   "source": [
    "**c)** To improve performance and memory usage (in particular for large datasets) it is important to use **categorical** columns whenever suitable. \n",
    "Are there any categorical column candidates? Explain your answer. \\\n",
    "Afterwards, convert these columns into categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"State\":'category', \"County\":'category'})\n",
    "# comment: I think county could actually be subcategories but this does not result in any memory improvements right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423b33c",
   "metadata": {},
   "source": [
    "Candidates for categorical columns are the Columns 'State' and 'County'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26df99",
   "metadata": {},
   "source": [
    "**d)** To select a good strategy to deal with missing data, it is important to get an overview over the general data distribution.\n",
    "Show the basic statistics for the dataset and create 6 boxplots for the following column groupings:\n",
    "\n",
    "    ['TotalPop', 'Men', 'Women', 'Citizen', 'Employed'],\n",
    "    ['Income', 'IncomePerCap'],\n",
    "    ['Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific'],\n",
    "    ['PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment', 'Poverty', 'ChildPoverty'],\n",
    "    ['Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome'],\n",
    "    ['MeanCommute']\n",
    "\n",
    "Can you spot any (severe) data quality problems, in particular, are there unrealistic values (also considering the semantics)?\n",
    "\n",
    "*Hint: Use the `df.describe()` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed03be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1467a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) # to suppress a warning (maybe because i don't use provided env?)\n",
    "df.boxplot(column=['TotalPop', 'Men', 'Women', 'Citizen', 'Employed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Income', 'IncomePerCap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0414dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment', 'Poverty', 'ChildPoverty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c890b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column=['MeanCommute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb903e4",
   "metadata": {},
   "source": [
    "the minimum value is 0 -> there are censuses without any entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af830fae",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "(In the following task you can assume that every NAN entry in the dataframe is actually a missing value. This can paritally be justified by the fact that pandas did not have problems inferring the \"proper\" datatypes (e.g., numbers types as string would result in an object column) and your subsequent check of the data types. Therefore, you can use `df.isna()` as a proxy indicator for missing values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91972aac",
   "metadata": {},
   "source": [
    "**e)** Simply filling missing entries is usually not a good idea. Therefore, you should first analyze the quantity of missing values and check for patterns of missing values.\n",
    "\n",
    "To this end, compute the following statistics on missing values:\n",
    "1) How many entries does the dataframe have? (To relate this to the number of entries missing)\n",
    "2) How many missing values do we have?\n",
    "3) How many rows have at least a single missing value?\n",
    "4) Count the number of missing values per column.\n",
    "5) Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values.\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total = len(df.columns)*len(df.index)\n",
    "num_missing = df.isna().sum().sum()\n",
    "print('Total number of entries: ', num_total)\n",
    "print('Total number of missing entries: ', num_missing)\n",
    "print('Percentage: ', num_missing/num_total*100)\n",
    "print()\n",
    "nan_sums_rows = df.isna().sum(axis=1)\n",
    "num_rows_with_missing = len(df.index) - nan_sums_rows.value_counts()[0]\n",
    "print('Number of rows with at least one missing value:', num_rows_with_missing)\n",
    "print()\n",
    "nan_sums_cols = df.isna().sum()\n",
    "print('Missing values per column:')\n",
    "print(nan_sums_cols)\n",
    "print()\n",
    "print('Missing values per row aggregated:')\n",
    "print(nan_sums_rows.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc710a89",
   "metadata": {},
   "source": [
    "In total 22469 (~0.89%) of the values are missing distributed over 2008 rows. Most values are missing for columns 'Income' and 'ChildPoverty'. All values for the columns 'State', 'County', 'TotalPop', 'Citizen' and 'Employed' are available. Most rows have no missing values. If there are missing values it seems to be either very little (1-3) or many at once (20 or 27)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd57102",
   "metadata": {},
   "source": [
    "**f)** We decide to **remove all rows from `df` where the total population is zero**. \\\n",
    "Given the preceding results, how do you evaluate this strategy? Try to motivate your argumentation by additional short analysis results (see hint for an inspiration).\n",
    "\n",
    "*Hint: It might be interesting to have a look at the rows with zero population. Afterwards, you can provide some analysis results that show that your (potential) observation generalizes to all rows with zero population.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c68231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows with zero population\n",
    "zero_pop = df.copy(deep=True)\n",
    "zero_pop = zero_pop[zero_pop.TotalPop == 0]\n",
    "print('There are {} rows with 0 population'.format(len(zero_pop)))\n",
    "print('number of NaN values per column:')\n",
    "zero_pop.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518e970",
   "metadata": {},
   "source": [
    "We observe that if the population is 0, most other values are missing. Therefore it makes sense to drop those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows were TotalPop is 0\n",
    "df = df.loc[df.TotalPop != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13598e45",
   "metadata": {},
   "source": [
    "## Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5efc4a",
   "metadata": {},
   "source": [
    "**g)** The previous analysis showed that there are missing values in the 'Men' and 'Women' columns.\\\n",
    "How would you impute these values? \\\n",
    "Motivate your approach and apply it to `df`.\n",
    "\n",
    "*Hint: Do not forget about the semantics of the columns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7527b",
   "metadata": {},
   "source": [
    "Idea: calculate the missing value for men from the value for total population and women and vice versa. We assume that each person is either a Man or a Woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfabae60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check if there are columns where both 'Men' and 'Women' is missing\n",
    "len(df[(df.Men.isna()) & (df.Women.isna())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4907f2",
   "metadata": {},
   "source": [
    "There are no columns were both values are missing so we can proceed as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Men'] = df['Men'].fillna(df.TotalPop - df.Women)\n",
    "df['Women'] = df['Women'].fillna(df.TotalPop - df.Men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32836684",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[['Men', 'Women']].isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef02564",
   "metadata": {},
   "source": [
    "**h)** Finally, impute the remaining missing values in `df` using the knn-imputation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa185f8c",
   "metadata": {},
   "source": [
    "1) Before you impute the remaining missing values, you should improve the data semantics consistency by turning the columns \n",
    "    \n",
    "        ['Men', 'Women', 'Citizen', 'Employed']\n",
    "    \n",
    "    into percentage scores as well. To this end, divide these values by the total population (i.e., 'TotalPop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077284e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['Men', 'Women', 'Citizen', 'Employed']:\n",
    "    df[key] = df[key] / df.TotalPop * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d959c1",
   "metadata": {},
   "source": [
    "2) Impute the missing values using the knn-imputation method.\n",
    "    To this end, apply the following steps:\n",
    "        1) Create a working copy `df_tmp` of your dataframe.\n",
    "        2) Drop the columns `['State', 'County']` from `df_tmp`. On the one hand, this makes the following steps easier because we only have to deal with numerical columns; on the other hand, an alternative one-hot encoding is also problematic as this will cause our feature dimensionality to explode!\n",
    "        3) Normalize the data in `df_tmp` (e.g., Standard score normalization). If the features have very different scales, even though we are mostly using percentages, knn can become very biased.\n",
    "        4) Impute the missing values considering five neighbors.\n",
    "        5) Invert the transformation applied upfront to enable more meaningful and intuitive visualizations.\n",
    "        6) Append the columns `['State', 'County']`\n",
    " \n",
    "In the end, `df` should not contain missing values and have columns `['State', 'County']`.\n",
    "\n",
    "*Hint: Be careful with the indices of your dataframes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create working copy\n",
    "df_tmp = df.copy(deep=True)\n",
    "# drop columns\n",
    "df_tmp = df_tmp.drop(['State', 'County'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data with standard scaler\n",
    "x = df_tmp.values # returns numpy array\n",
    "standard_scaler = StandardScaler()\n",
    "x_scaled = standard_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values via knn-imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "x_imputed = knn_imputer.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7117432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale data\n",
    "x_rescaled = standard_scaler.inverse_transform(x_imputed)\n",
    "df_tmp = pd.DataFrame(x_rescaled, index=df_tmp.index, columns=df_tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88088b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp['State'] = df['State']\n",
    "df_tmp['County'] = df['County']\n",
    "df = df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5339e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'State' in df.columns\n",
    "assert 'County' in df.columns\n",
    "assert df.isna().sum().sum() == 0\n",
    "assert df['Hispanic'].min() > -0.01\n",
    "assert df['Hispanic'].max() < 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee1b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a62a12",
   "metadata": {},
   "source": [
    "**i)** In the final preprocessing step, you should integrate one additional source of data into the preprocessed dataframe `df`. \n",
    "As the data has a natural geospatial dimension, you are going to endow each tract with its geographic coordinate.\n",
    "To this end, load **coordinates.csv**. Integrate the two data sources exploiting the correspondence between 'CensusTract' and 'GEOID'.\n",
    "Finally, drop the 'USPS' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load coordinates dataset\n",
    "coords = pd.read_csv('./dataset/coordinates.csv', index_col='GEOID')\n",
    "# join both datasets\n",
    "df = df.join(coords)\n",
    "# drop 'USPS'\n",
    "df = df.drop(['USPS'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'Men' in df.columns\n",
    "assert 'County' in df.columns\n",
    "assert 'INTPTLONG' in df.columns\n",
    "assert 'INTPTLAT' in df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216cc6a6",
   "metadata": {},
   "source": [
    "# Q2 - Visualization (15 points)\n",
    "In this task, you will analyze the data that you preprocessed in question 1 (**census_data.csv**). In particular you will analyze income-related aspects, using different means of visualization.\n",
    "\n",
    "Start with the following preprocessed and integrated dataframe `df`. \\\n",
    "Note that it has a similar structure to the dataframe that you should obtain from the previous task, however, the values have been modified!\n",
    "\n",
    "**Library usage:** This notebook imports a couple visualization libraries that have a significat overlap in terms of functionalities. Therefore, you are free to use any of these libraries (and those in the environment in general) to implement the following questions as long as your resulting plot compilies with the explicitly mentioned requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71419092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./dataset/df_vis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef00c297",
   "metadata": {},
   "source": [
    "**a)** Visualize two histograms for 'Income' and 'IncomePerCap' in a **single plot**. Compare the two distributions; what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400eae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Income', 'IncomePerCap']].plot.hist(bins=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325fd434",
   "metadata": {},
   "source": [
    "It is observable that the income is in general higher than the income per capita (PCI) and the PCI is more spread. This could be because counties with a high population have an overall high income but the income averaged over the population is not so high. \n",
    "\n",
    "\n",
    "*income is median household income, pci is totalincome/capita so mean income*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e6641",
   "metadata": {},
   "source": [
    "## Aggregation for Visualization Pruposes\n",
    "**b)** As the data contains too many rows for per-row visualizations, you should aggregate the data further before creating more interesting visualizations.\n",
    "The following function will do the job for your; however, why didn't we simply run `groupby(...).mean()` to get the results for the columns\n",
    "specified in `l_col`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1305be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_aggregation(df):\n",
    "    l_col = ['Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Citizen', 'Income', 'IncomePerCap', \n",
    "             'Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', \n",
    "             'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed', \n",
    "             'FamilyWork', 'Unemployment', 'INTPTLAT', 'INTPTLONG']\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp.loc[idx[:, l_col]] = df_tmp.loc[idx[:, l_col]].mul(df_tmp['TotalPop'], axis=0)\n",
    "    df_tmp = df_tmp.groupby(['State', 'County'], observed=True).sum()\n",
    "    df_tmp.loc[idx[:, l_col]] = df_tmp.loc[idx[:, l_col]].div(df_tmp['TotalPop'], axis=0)\n",
    "    \n",
    "    return df_tmp\n",
    "\n",
    "# multiply by totalPop (compute absolute value)\n",
    "# group by state county\n",
    "# divide by totalpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = my_aggregation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83099ef",
   "metadata": {},
   "source": [
    "When we group multiple regions with different populations, the mean over the statistics depend on the population of the regions so that taking the mean over the percentages is only true, when the population of the regions is the same. This assumption however doen't (always) hold.\n",
    "\n",
    "*why would the same region have different number of totalPop?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd4125",
   "metadata": {},
   "source": [
    "**c)** Next, you should create an overview over column correlations particularly considering high/medium/and low incomes.\n",
    "\n",
    "1) Create a copy `df_plot` of the aggregated dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3259cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9287c5f",
   "metadata": {},
   "source": [
    "2) Append a column 'IncomeClass' to `df_plot` containing the 'Income' categories based on the following inter-percentile ranges:\n",
    "    - low' iff the income is less than the 33% income percentile\n",
    "    - 'medium' if the income is between the 33% and 66% percentile, and \n",
    "    - 'high' iff the income is above the 66% percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_plot.assign(IncomeClass=pd.qcut(df_plot['Income'], \n",
    "                                             [0, 0.33, 0.66, 1], \n",
    "                                             ['low', 'medium', 'high']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2c43f",
   "metadata": {},
   "source": [
    "3) Project the dataframe on the columns that contain percent values (for the sake of readability), that is:\n",
    "\n",
    "        ['Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Poverty', 'ChildPoverty', \n",
    "        'Professional', 'Service', 'Office', 'Construction', 'Production',\n",
    "        'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome',\n",
    "        'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed',\n",
    "        'FamilyWork', 'Unemployment', 'IncomeClass']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac57485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_plot[['Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction', 'Production','Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome','MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed','FamilyWork', 'Unemployment', 'IncomeClass']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09068393",
   "metadata": {},
   "source": [
    "4) Create a parallel coordinates diagram that uses the 'IncomeClass' for coloring the lines. Rotate the x-axis labels by 90° to make them easier to read.\n",
    "Briefly discuss your results. Do you observe any correlations? Please explain.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0253bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.pyplot.figure(figsize=(25,12), dpi=120)\n",
    "plot = pd.plotting.parallel_coordinates(df_plot, 'IncomeClass', alpha=0.3, color=[\"green\", \"violet\", \"blue\"])\n",
    "plot.tick_params(axis='x', labelrotation=90)\n",
    "plt.show()\n",
    "# comment: rotate label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4824fc1",
   "metadata": {},
   "source": [
    "Most observations aren't interesting and obvious like the negative correlation between men and women. Also the correlations in groups of the same category like \"drive\" and \"Carpool\" are obvious because if many people drive, they don't make use of carpools, which implies a negative correlation in this case.\n",
    "Meaningful Obervations: \n",
    "- positive correlation between the amount of production in an area and people driving\n",
    "- negative correlation between Poverty and Professional Workers, same for child poverty\n",
    "- positive correlation between Poverty and service workers\n",
    "- positive correlation between poverty and amount of black people\n",
    "- negative correlation between unemployment and amount of white people + asian people\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef82b6",
   "metadata": {},
   "source": [
    "## Advanced Visualization: Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee5eb3",
   "metadata": {},
   "source": [
    "**d)** In this task, you are going to create an advanced visualization that exploits the geospatial nature of the data, that is, you will project the average 'Income' of each county and its population onto a map of the USA. \\\n",
    "You can use the following code to create a suitable map extend.\n",
    "\n",
    "    ax.set_extent([-125, -66.5, 20, 50], ...)\n",
    "        \n",
    "Given this instance, plot one marker (e.g., circular marker) for each row in our aggregated dataset onto the map.\n",
    "The color encoding should show the average 'Income' of the corresponding county, while the size should be chosen according to its population ('TotalPop'). If you want to modify the dataframe, create a **working copy** beforehand.\n",
    "\n",
    "What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e41586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "# Create projection\n",
    "proj = ccrs.PlateCarree()\n",
    "\n",
    "# Colormap\n",
    "cmap = mpl.cm.get_cmap('RdYlGn')\n",
    "# Normalization\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=df['Income'].max())\n",
    "# Get colors according to color map specification above\n",
    "np_col = cmap(norm(df['Income']))\n",
    "\n",
    "# Compute the circle size\n",
    "# To show you \n",
    "size_scaler = MinMaxScaler()\n",
    "np_size = 600 * size_scaler.fit_transform(df[['TotalPop']])\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection=proj)\n",
    "ax.add_feature(cfeature.BORDERS)\n",
    "ax.add_feature(cfeature.STATES)\n",
    "ax.set_extent([-125, -66.5, 20, 50])\n",
    "ax.scatter(df['INTPTLONG'], df['INTPTLAT'], color=np_col, s=np_size, marker='o')\n",
    "\n",
    "# Add colorbar to the plot\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_horizontal(size='5%', pad=0.2, axes_class=plt.Axes)\n",
    "fig.add_axes(cax)\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "             cax=cax, orientation='vertical')\n",
    "cbar.set_label('Median Household Income', rotation=270)\n",
    "# Figure title\n",
    "fig.suptitle('Median Household Income in Counties of the US')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ddfed",
   "metadata": {},
   "source": [
    "We can observe that in most larger cities, the income is high. In the inner country, with cities with lower populations, the income is in general lower. An exception is the state of Florida, where the overall income seems to be low, even though there are some big cities like Orlando and Miami. We can also see that more people live in the East while especially northern states have low populations.\n",
    "\n",
    "*arent there quite some big green points in FLorida?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df9287",
   "metadata": {},
   "source": [
    "# Q3 - Frequent Itemsets and Association Rules (12 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb215f",
   "metadata": {},
   "source": [
    "## Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7452623",
   "metadata": {},
   "source": [
    "**a)** Carry out some preprocessing steps before starting the analysis:\n",
    "\n",
    "1) Load the `customer_data.csv`.\n",
    "\n",
    "2) Select 90% of the `customer_data` dataset by random sampling. Use the matriculation number of one of the group members as seed.\n",
    "\n",
    "3) After completing this preprocessing step, export your final dataset as `customer_data_2.csv` and use it for the next steps of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a71e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = pd.read_csv('./dataset/customer_data.csv')\n",
    "customer_data_sample = customer_data.sample(frac=0.9, random_state=427492)\n",
    "customer_data_sample.to_csv('./dataset/customer_data_2.csv')\n",
    "customer_data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43b077",
   "metadata": {},
   "source": [
    "**b)** In this part, we want to get to know our customers by looking at the typical shared characteristics (e.g. \"Married customers in their 40s like wine\"). This would correspond to the itemset {Married, 40s, Wine}. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52477ea",
   "metadata": {},
   "source": [
    "1) Create a new dataframe called `customer_data_onehot` such that rows correspond to customers (as in the original data set) and columns correspond to the categories of each of the ten categorical attributes in the data. The new dataframe should only contain boolean values (True/False or 0/1s) such that the value in row $i$ and column $j$ is True (or 1) if and only if the attribute value corresponding to the column $j$ holds for the customer corresponding to row $i$. Display the dataframe.\n",
    "\n",
    "*Hint: For example, for the attribute \"Education\" there are 5 possible categories: 'Graduation', 'PhD', 'Master', 'Basic', '2n Cycle'. Therefore, the new dataframe must contain one column for each of those attribute values.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data_sample = pd.read_csv('./dataset/customer_data_2.csv')\n",
    "customer_data_onehot = pd.get_dummies(customer_data_sample)\n",
    "customer_data_onehot = customer_data_onehot.rename(columns={'Unnamed: 0': 'ID'}).set_index(\"ID\")\n",
    "customer_data_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcf9a2",
   "metadata": {},
   "source": [
    "2) Use the apriori algorithm to find the frequent itemsets with **min_support = 0.3** from the `customer_data_onehot` dataframe. Show the frequent itemsets that contain at least **3** items.\n",
    "\n",
    "*Hint: The apriori algorithm of mlxtend needs a dataframe containing only boolean values as input.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(customer_data_onehot, min_support = 0.3, use_colnames = True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x)) \n",
    "frequent_itemsets = frequent_itemsets[frequent_itemsets['length'] >= 3]\n",
    "display(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea55075",
   "metadata": {},
   "source": [
    "**c)** In the following we will investigate the effect of using the apriori property when determining the candidates for the frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6018b320",
   "metadata": {},
   "source": [
    "1) Implement the following join- and prune steps of the Apriori algorithm: \\\n",
    "   **join function:** a function that, given the frequent itemsets of size k, generates and yields a list of itemsets of size k+1. Only itemsets that share exactly k elements should be merged. \\\n",
    "   **prune function:** Given the set of candidate itemsets of size k+1 and the set of frequent itemsets of size k, remove the candidate sets that contain an infrequent subset of size k and return the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b760e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(itemsets_k):\n",
    "    k = len(itemsets_k[0])\n",
    "    extended_sets = []\n",
    "\n",
    "    for outer_set in itemsets_k:\n",
    "        for inner_set in itemsets_k:\n",
    "            if len(outer_set.intersection(inner_set)) == k-1:\n",
    "                extended_sets.append(set(outer_set).union(set(inner_set)))\n",
    "\n",
    "    return extended_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(itemsets_k, itemsets_k1):\n",
    "    k = len(itemsets_k[0])\n",
    "    freq_itemsets = []\n",
    "\n",
    "    for freq_set in itemsets_k1:\n",
    "        is_frequent = True\n",
    "\n",
    "        for item in freq_set:\n",
    "            set_copy = freq_set.copy()\n",
    "            set_copy.remove(item)\n",
    "            if set_copy not in itemsets_k:\n",
    "                is_frequent = False\n",
    "                break\n",
    "            \n",
    "        if is_frequent:\n",
    "            freq_itemsets.append(freq_set)\n",
    "\n",
    "    return freq_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7d3b5",
   "metadata": {},
   "source": [
    "2) To see the effect of the apriori property, compare the number of candidate itemsets of size 4 obtained with and without pruning from the itemsets of size three for different values for min_support. To this end, generate a list of tuples *(min_sup, C4_size, C4_size_pruned, L4_size)* as follows:\n",
    "\n",
    "For $\\textrm{min_support} \\in [0.1,0.2,...,0.8,0.9,1]$, repeat:\n",
    "\n",
    "1. Obtain all frequent itemsets of size three using the apriori algorithm.\n",
    "\n",
    "2. Using the result from 1., generate all itemsets of size four by applying your **join function** $\\rightarrow$ C4_size.\n",
    "\n",
    "3. Prune the result from 2. using your **prune function** $\\rightarrow$ C4_size_pruned.\n",
    "\n",
    "4. Compute the frequent itemsets of size four by using the apriori algorithm $\\rightarrow$ L4_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daadc73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vals=[0.02, 0.04, 0.08, 0.16, 0.32]\n",
    "plot_data = []\n",
    "\n",
    "for support in support_vals:\n",
    "    frequ_itemsets = apriori(customer_data_onehot, use_colnames = True, min_support=support)\n",
    "    frequ_itemsets['length'] = frequ_itemsets['itemsets'].apply(lambda x: len(x)) \n",
    "\n",
    "    frequ_itemsets_size3 = (frequ_itemsets[frequ_itemsets['length'] == 3])['itemsets'].tolist()\n",
    "    frequ_itemsets_size4 = (frequ_itemsets[frequ_itemsets['length'] == 4])['itemsets'].tolist()\n",
    "\n",
    "    if frequ_itemsets_size3:\n",
    "        itemsets_size4_dirt = join(frequ_itemsets_size3)\n",
    "        itemsets_size4_clean = prune(frequ_itemsets_size3, itemsets_size4_dirt)\n",
    "        \n",
    "        plot_data.append([support, len(itemsets_size4_dirt), len(itemsets_size4_clean), len(frequ_itemsets_size4)])\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data, columns=['support', 'before prune', 'after prune', 'apriori'])\n",
    "display(df_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969fec0",
   "metadata": {},
   "source": [
    "3) Plot the number of candidate sets with and without pruning and the number of frequent itemsets of size four against the corresponding min_sup value. Interpret the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "for col in ['before prune', 'after prune', 'apriori']:\n",
    "    df_plot.plot(kind='line',x='support',y=col, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacdd90",
   "metadata": {},
   "source": [
    "We can obverse that pruning can reduce the number of itemsets by a significant amount. However the actual target what we get with the apriori algorithm is still a lot lower, meaning that the procedure is not sufficient to make predictions on the actual number of targets. All lines converge to zero which makes sense since the more items we take into account, the less fullfill the condition of being contained in a set after the apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d53e9",
   "metadata": {},
   "source": [
    "d) Use the FP-Growth algorithm to obtain all frequent itemsets with **min_support = 0.3** from `customer_data_onehot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = fpgrowth(customer_data_onehot, min_support=0.3, use_colnames = True)\n",
    "display(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1118f",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475824f2",
   "metadata": {},
   "source": [
    "**d)** In the following, you should generate association rules from the frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86658d02",
   "metadata": {
    "tags": []
   },
   "source": [
    "1) Using only the frequent itemsets with min_support=0.3, generate different association rules using minimum confidence equal to 0.6 as a metric. Show the association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_rules = arule(frequent_itemsets, metric = 'confidence', min_threshold = 0.6)\n",
    "display(assoc_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0035e9",
   "metadata": {},
   "source": [
    "2) From the association rules obtained in task (d) 1), provide the three rules with the highest lift. Comment on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528336a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(assoc_rules.sort_values('lift', ascending=False))[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b09c4",
   "metadata": {},
   "source": [
    "Since the lift for all 3 rules is over 1 or even over 2, it means that there is a positive correlation between antecedents and consequents. So the occurence of the antecedents implies that the consequents are also likely to occur. The relatively high confidence of over 78% for all three values confirm that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3d4e5",
   "metadata": {},
   "source": [
    "# Q4 - Text Mining (15 points)\n",
    "In this question, you will use the scripts of some Harry Potter movies. First, you will try to predict the character given a line in the script. Afterwards, using N-grams, you will generate sentences for some of the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc659ba",
   "metadata": {},
   "source": [
    "**a)** In this part, you will preprocess and reconstruct the data to make it suitable for the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba32e37",
   "metadata": {},
   "source": [
    "1) Load each of the datasets <b>hp_1.csv</b>, <b>hp_2.csv</b>, and <b>hp_3.csv</b> into its own dataframe and show the set of characters (here: a fictional character) appearing in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "hp_1 = pd.read_csv('./dataset/hp_1.csv')\n",
    "hp_2 = pd.read_csv('./dataset/hp_2.csv')\n",
    "hp_3 = pd.read_csv('./dataset/hp_3.csv')\n",
    "\n",
    "# strip leading and trailing whitespaces and convert to lower case \n",
    "hp_1 = hp_1.applymap(lambda x : x.strip().lower())\n",
    "hp_2 = hp_2.applymap(lambda x : x.strip().lower())\n",
    "hp_3 = hp_3.applymap(lambda x : x.strip().lower())\n",
    "\n",
    "# compute characters that appear in each of the datasets\n",
    "from functools import reduce\n",
    "print(reduce(np.intersect1d, (hp_1['Character'], hp_2['Character'], hp_3['Character'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250046a",
   "metadata": {},
   "source": [
    "2) Merge the three datasets into a single dataframe called `hp_df` that comprises only the lines spoken from one of the four characters *Harry, Hermione, Dumbledore, and Snape*. Your new dataframe must contain two columns: one for the (four) characters and the other for the lines. You can name those columns \"Character\" and \"Sentence\" as in the original data. \\\n",
    "    Make sure that `hp_df` contains a single unique spelling for each of the characters. \\\n",
    "    Make sure that `hp_df` includes all lines (here: script lines) of a character even if this character is spelled slightly differently (e.g., Dumbledore or dumbledore) in the original dataset. \\\n",
    "    Show the first few lines of your dataframe.\n",
    "    \n",
    "*Hint: Be aware of white space characters!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf353ebe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hp_df = pd.concat([hp_1, hp_2, hp_3])\n",
    "hp_df = hp_df.loc[hp_df['Character'].isin(['harry', 'hermione', 'dumbledore', 'snape'])]\n",
    "hp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a196a9f",
   "metadata": {},
   "source": [
    "Note: We neglect the typo in 'hermoine' and examples with multiple people. The number of occurences of these are very low so this should be okay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4eb3a9",
   "metadata": {},
   "source": [
    "3) Create the `hp_sampled` dataset which includes 90% of the `hp_df` data. Use the matriculation number of one of the group members as seed. Export the sampled dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_sampled = hp_df.sample(frac=0.9, random_state=427492)\n",
    "hp_sampled.to_csv('./dataset/hp_sampled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2492a0",
   "metadata": {},
   "source": [
    "**b)** In this part, you are going to train a classifier that, given a line from the script, predicts the character. For each character, the data contains many sentences belonging to that character. Note that sometimes the \"Sentence\" column in the original dataset contains more than one sentence. The set of sentences for each character should be seen as the set of example documents belonging to that character (the class). Each individual sentence is a single document. The whole corpus consists of all the individual sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4434e3",
   "metadata": {},
   "source": [
    "1) Create a new dataframe called `hp_processed` from the dataframe `hp_sampled` such that the new dataframe contains again the columns \"Character\" and \"Sentence\", but every entry in the \"Sentence\" column must be a single sentence. Display the shape of the dataframe and compare it to the shape of `hp_sampled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cedff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename index so no id appears multiple times\n",
    "hp_processed = hp_sampled.set_axis(range(len(hp_sampled)))\n",
    "\n",
    "s = hp_processed.Sentence.map(sent_tokenize).apply(pd.Series, 1).stack()\n",
    "s.index = s.index.droplevel(-1) # to line up with df's index\n",
    "s.name = 'Sentence' # needs a name to join\n",
    "del hp_processed['Sentence']\n",
    "hp_processed = hp_processed.join(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d37b1b",
   "metadata": {},
   "source": [
    "2) Split the preprocessed data `hp_processed` into training (80%) and test (20%) data preserving the distribution based on \"Character\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(hp_processed, test_size=0.2, random_state=427492, stratify=hp_processed.Character)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f233db",
   "metadata": {},
   "source": [
    "3) Preprocess the training and test corpus (to lowercase, no punctuation, tokenization, lemmatization, and stopword removal) and obtain a boolean document-term matrix (i.e, each row in the matrix contains only 1s and 0s depending on whether a particular word appears in a sentence or not). Train a logistic classifier on the training corpus with the character as target feature. Use the classifier to predict the character of the sentences in the test corpus and show its accuracy on the test corpus. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec242ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stop_list = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(sentences):\n",
    "    corpus = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        filtered = [word.lower() for word in tokenized if word.lower() not in stop_list]\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in filtered]\n",
    "        corpus.append(lemmatized)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa72e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = preprocess(train.Sentence)\n",
    "corpus_test = preprocess(test.Sentence)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(pd.Series([' '.join(s) for s in corpus_train]))\n",
    "\n",
    "train_count_matrix = count_vect.transform(pd.Series([' '.join(s) for s in corpus_train]))\n",
    "test_count_matrix = count_vect.transform(pd.Series([' '.join(s) for s in corpus_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc5feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating classifier')\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "train_vectors = train_count_matrix\n",
    "train_target = train.Character\n",
    "\n",
    "test_vectors = test_count_matrix\n",
    "test_target = test.Character\n",
    "\n",
    "classifier = classifier.fit(X=train_vectors, y=train_target)\n",
    "\n",
    "print('Evaluating model on test set')\n",
    "\n",
    "print('Classifier accuracy on test set:')\n",
    "print(classifier.score(test_vectors, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd8928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = classifier_doc2vec.predict(test_vectors)\n",
    "confusion_matrix(test.Character, predictions, labels=['harry', 'hermione', 'dumbledore', 'snape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12deeaad",
   "metadata": {},
   "source": [
    "The performance of the logistic classifier is pretty poor with 56% accuracy. Especially considering, that the classifier learns that the character Harry has more occurences in the data. This is probably due to the sparseness of the data. It is very unlikely that similar sentences appear multiple times, especially since similar words cannot be determined from the count matrix representation. Also most sentences have barely any words because of the stopwaord removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9639218",
   "metadata": {},
   "source": [
    "4) Next, you are going to perform the same predicting task based on doc2vec.\n",
    "\n",
    "1. Preprocess the training corpus (to lowercase, no punctuation, tokenization, lemmatization, and stopword removal). \n",
    "2. Create a doc2vec model to reduce the dimension of the document vector. Choose a vector size 4-8 and ignore all words whose count is lower than 3.\n",
    "3. Train the doc2vec model on the training data (thus creating an embedding).\n",
    "4. Use the created embedding to convert the training set to a set of document vectors.\n",
    "5. Train a logistic classifier on the train data with the character as target feature.\n",
    "6. Show the accuracy of prediction on the test data and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c985a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TaggedDocument\n",
    "tagged_docs_train = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_train)]\n",
    "\n",
    "# determining parameters of the model   \n",
    "model = Doc2Vec(vector_size=8, min_count=3)\n",
    "\n",
    "# building the vocabulary    \n",
    "model.build_vocab(tagged_docs_train)\n",
    "\n",
    "training_examples = model.corpus_count\n",
    "\n",
    "# document embedding, train the model on the training examples (docs)\n",
    "model.train(corpus_iterable=tagged_docs_train, total_examples=training_examples, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating classifier')\n",
    "\n",
    "classifier_doc2vec = LogisticRegression()\n",
    "\n",
    "train_vectors = [model.infer_vector(doc) for doc in corpus_train]\n",
    "train_target = train['Character']\n",
    "\n",
    "test_vectors = [model.infer_vector(doc) for doc in corpus_test]\n",
    "test_target = test['Character']\n",
    "\n",
    "classifier_doc2vec = classifier_doc2vec.fit(X=train_vectors, y=train_target)\n",
    "\n",
    "print('Evaluating model on test set')\n",
    "\n",
    "print('doc2vec - Classifier accuracy on test set:')\n",
    "print(classifier_doc2vec.score(test_vectors, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e9af3",
   "metadata": {},
   "source": [
    "**c)** For the following tasks use the `hp_processed` (the data before splitting into training and test data).\n",
    "\n",
    "1) For each character, create a list containing all sentences of that character.\n",
    "    For each character separately, build a bigram language model using MLE. Do not perform stemming and stopword removal for this task, but apply other preprocessing steps such as to lowercase, no punctuation, and tokenization. Use both right and left padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a30877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_simple(sentences):\n",
    "    corpus = []\n",
    "    for sentence in sentences:\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        filtered = [word.lower() for word in tokenized]\n",
    "        corpus.append(filtered)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = ['harry', 'hermione', 'dumbledore', 'snape']\n",
    "\n",
    "hp_data = [hp_processed[hp_processed.Character == c] for c in characters]\n",
    "hp_corpora = [preprocess_simple(d.Sentence) for d in hp_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lm(n,corpus):\n",
    "    # the padded_everygram_pipeline goes through the corpus, applies left and right padding to the sentences (adding <s> and </s>)\n",
    "    # and obtains the tuples of a given order together with the vocabulary\n",
    "    padded_tuples, vocab = padded_everygram_pipeline(n, corpus)\n",
    "    # generate (an empty) ngram language model for some n>0\n",
    "    lm = MLE(n)\n",
    "    # generate probabilities (model) given the list of n-grams and the vocabulary\n",
    "    lm.fit(padded_tuples, vocab)\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [build_lm(2, corpus) for corpus in hp_corpora]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191209bd",
   "metadata": {},
   "source": [
    "2) For each character, use the created language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(characters)):\n",
    "    print('{}:\\t {}'.format(characters[i], ' '.join(bigrams[i].generate(10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b10a1",
   "metadata": {},
   "source": [
    "3) Build a 4-gram model with the same data as in the previous task. Use both right and left padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourgrams = [build_lm(4, corpus) for corpus in hp_corpora]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787c90b",
   "metadata": {},
   "source": [
    "4) For each character, use the created 4-gram language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(characters)):\n",
    "    print('{}:\\t {}'.format(characters[i], ' '.join(fourgrams[i].generate(10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef869fac",
   "metadata": {},
   "source": [
    "5) Compare the sentences generated by the bigram model with the sentences generated by the 4-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c209e8",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fa9e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q5 - Process Mining (23 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a008319",
   "metadata": {},
   "source": [
    "In this task, we consider a simulated process of students that participate in an online course.\n",
    "The course comprises 6 batches of lecture material as well as a mandatory assignment to be delivered in two parts. (Note that in this process, it is not required to achieve a certain score in the assignment in order to be admitted to the exam.)\n",
    "\n",
    "While there are strict deadlines for the assignment and the exam, there is only a recommended schedule for the lecture material (i.e., consume material in order).\n",
    "\n",
    "The system logs for every student, among other activities, when he downloads a certain lecture material batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531f183",
   "metadata": {},
   "source": [
    "## Loading the Data and Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807f560",
   "metadata": {},
   "source": [
    "**a)** Load the data **log.csv** and create a PM4Py event log. In doing so, use the following column mapping:\n",
    " - 'Activity' is the activity key\n",
    " - 'Student' is the case ID\n",
    " - 'Timestamp' is the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51969b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "log = pd.read_csv('dataset/log.csv')\n",
    "df = pm4py.format_dataframe(log, case_id='Student', activity_key='Activity', timestamp_key='Timestamp')\n",
    "log = log_converter.apply(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d232e3f",
   "metadata": {},
   "source": [
    "**b)** Compute the following basic information:\n",
    "- Number of events\n",
    "- Number of cases\n",
    "- Earliest timestamp\n",
    "- Latest timestamp\n",
    "- Number of trace variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "print('number of events: {}'.format(df.shape[0]))\n",
    "print('number of cases: {}'.format(len(log)))\n",
    "print('earliest timestamp: {}'.format(df.sort_values('time:timestamp')['time:timestamp'].iloc[0]))\n",
    "print('latest timestamp: {}'.format((df.sort_values('time:timestamp', ascending=False))['time:timestamp'].iloc[0]))\n",
    "print('number of trace variants: {}'.format(len(variants_filter.get_variants(log))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9c03c",
   "metadata": {},
   "source": [
    "**c)** Usually, it is insightful to have a look at the distribution of the variants in terms of how often a certain variant is present in the log.\n",
    "Therefore, create a **scatter plot** that shows the distribution of the variants as follows:\n",
    "- x-axis: The variants (in ascending order of their support)\n",
    "- y-axis: Frequency of the variant in the log (total or relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "variants_count = case_statistics.get_variant_statistics(log)\n",
    "vc_df = (pd.DataFrame.from_dict(variants_count)).sort_values('count', ascending=True)\n",
    "var_freq = [(len(vc_df)-index, row['count']) for index, row in vc_df.iterrows()]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(*zip(*var_freq))\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('variant')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "# comment: change type to scatter plot, but then the labels are unreadable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79825e",
   "metadata": {},
   "source": [
    "**d)** While the variant distribution shows potential standard process executions in terms of the activity ordering, the distribution of the case durations shows the typical timeframe of cases.\n",
    "Create a histogram plot over the case durations. For the sake of readability, make sure that the x-axis labels (in this case the case durations) have an easily readable format, that is, your x-axis labels should look like this:\n",
    "<br></br>\n",
    "<div>\n",
    "<img src=\"templates/caseDurationXAxis.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cec96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "td = datetime.timedelta(seconds=13700502.0)\n",
    "print(td)\n",
    "\n",
    "case_dur = pd.DataFrame.from_dict(case_statistics.get_cases_description(log))\n",
    "case_dur_t = case_dur.transpose()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "chart = plt.hist(case_dur_t['caseDuration'])\n",
    "\n",
    "def timeTicks(x, pos):                                                                                                                                                                                                                                                         \n",
    "    d = datetime.timedelta(seconds=x)\n",
    "    return str(d)\n",
    "formatter = mpl.ticker.FuncFormatter(timeTicks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.xaxis.set_major_formatter(formatter)  \n",
    "\n",
    "#, ax=ax, x=\"datetime\",y=\"Duration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6b50",
   "metadata": {},
   "source": [
    "## Discovery and Conformance Checking\n",
    "Next, you are going to discover process models for different perspectives on the process. Moreover, you will evaluate how well the process models can represent the behavior present in the log (i.e., the fitness of the models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ae3c6",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0baabd",
   "metadata": {},
   "source": [
    "**e)** Before discovering models, create three addtional perspectives onto the process by creating three additional event log from the log loaded in a):\n",
    "\n",
    "1. Log containing only 30% of the most frequent traces (**log_varaint03**)\n",
    "2. Log containing only 50% of the most frequent traces (**log_varaint05**)\n",
    "3. Log containing only students that take the exam, that is, cases that end with 'Exam' (**log_exam**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "log_varaint03 = variants_filter.filter_log_variants_percentage(log, percentage=0.3)\n",
    "log_varaint05 = variants_filter.filter_log_variants_percentage(log, percentage=0.5)\n",
    "from pm4py.algo.filtering.log.end_activities import end_activities_filter\n",
    "log_exam = end_activities_filter.apply(log, [\"Exam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d8188c",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Inductive Miner and Replay Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a5caa",
   "metadata": {
    "tags": []
   },
   "source": [
    "**f)** To get a better understanding of the processes in our four event logs (base log + three additional logs), create processes models using the **Inductive Miner**.\n",
    "Concretely, for each of the 4 event logs, create two process models using the Inductive Miner with noise threshold **0 and 0.2**. Moreover, to access how well the model presents the logged behavior, apply conformance checking in terms of token-based replay to the model and the log from which it has been mined. Visualize each model as a process tree and as the corresponding Petri net.\n",
    "\n",
    "In total, your cells should output 8 conformance scores, 8 process trees, and 8 Petri nets (for each log + noise threshold combination). Make sure that it is clear which model and conformance score belongs to log and parameter configuration.\n",
    "\n",
    "For example, your output can look like this\n",
    "#### Log: Base\n",
    "##### IM threshold 0\n",
    ">Fitness score\n",
    "\n",
    ">Picture of the process tree\n",
    "\n",
    ">Picture of the Petri net\n",
    "\n",
    "##### IM threshold 0.2\n",
    ">Fitness score\n",
    "\n",
    ">Picture of the process tree\n",
    "\n",
    ">Picture of the Petri net\n",
    "\n",
    "**Describe your results**. How well do the models fit and, in particular, how do the models for\n",
    "- log_variant05 and log (base log)\n",
    "- log_variant05 and log_variant03\n",
    "- log base and log_exam\n",
    "\n",
    "differ in terms of the behavior that they allow?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6378e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "for item in [(log, 'full log'), (log_varaint03, '30% most frequent'), (log_varaint05, '50% most frequent'), (log_exam, 'ends with exam')]:\n",
    "    print('log: ' + item[1])\n",
    "    proc_tree = pm4py.discover_process_tree_inductive(item[0], noise_threshold=0)\n",
    "    gviz = pt_visualizer.apply(proc_tree, parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"})\n",
    "    pt_visualizer.view(gviz)\n",
    "\n",
    "    net, im, fm = pt_converter.apply(proc_tree)\n",
    "    pm4py.view_petri_net(net, im, fm, format='png')\n",
    "    fitness = replay_fitness_evaluator.apply(item[0], net, im, fm, variant=replay_fitness_evaluator.Variants.TOKEN_BASED)\n",
    "    display(fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7123f30c",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- 30% most frequent visit almost all lectures and take the assignments and the exam\n",
    "- 50% most frequent: more students skip events\n",
    "- all logs: Looks similar to the 50% most frequent logs\n",
    "- exam at the end: also looks similar to the 50% most frequent logs but there are a few less skip connections\n",
    "The fitness for all the logs is the same so this value isn't interesting for observing differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd13f8",
   "metadata": {},
   "source": [
    "### Paths of Excellence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46fc44",
   "metadata": {},
   "source": [
    "**g)** As lectures are in a constant urge to improve their courses in a way that participants learn as much as possible ;), you are facing the research question to identify *paths of excellence*.\n",
    "In particular, you should identify how the studying behavior differs between excellent students (**final exam score greater than or equal to 85**) and non-excellent students (**final exam score less than 85**).\n",
    "Try to answer this research questions using techniques from the preceding Process Mining questions. \n",
    "\n",
    "*Hint: There is no single unique solution (e.g., in terms of parameter choice); therefore, it suffices if your \"design choices\" are reasonble.* \\\n",
    "*Hint: The final exam score is point score that is associated with the \"Exam\" event.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.objects.petri_net.obj import PetriNet, Marking\n",
    "\n",
    "\n",
    "\n",
    "log_excellence = attributes_filter.apply_numeric(log_exam, 85, 100,\n",
    "                                             parameters={attributes_filter.Parameters.ATTRIBUTE_KEY: \"Points\",\n",
    "                                                         attributes_filter.Parameters.STREAM_FILTER_KEY1: \"concept:name\",\n",
    "                                                         attributes_filter.Parameters.STREAM_FILTER_VALUE1: \"Exam\"})\n",
    "\n",
    "log_non_excellence = attributes_filter.apply_numeric(log_exam, 0, 84,\n",
    "                                             parameters={attributes_filter.Parameters.ATTRIBUTE_KEY: \"Points\",\n",
    "                                                         attributes_filter.Parameters.STREAM_FILTER_KEY1: \"concept:name\",\n",
    "                                                         attributes_filter.Parameters.STREAM_FILTER_VALUE1: \"Exam\"})\n",
    "\n",
    "for item in [('log_excellence', log_excellence), ('log_non_excellence', log_non_excellence)]:\n",
    "    print('log: ' + item[0])\n",
    "    proc_tree = pm4py.discover_process_tree_inductive(item[1], noise_threshold=0)\n",
    "    net, im, fm = pt_converter.apply(proc_tree)\n",
    "    parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "    gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=item[1])\n",
    "    dfg_visualization.view(gviz)\n",
    "\n",
    "print(len(log_non_excellence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aeba52",
   "metadata": {},
   "source": [
    "It is clearly evident that the students who take an excellent exam attend the lectures in their intended order and complete the assignments. Likewise, each of these students attended the final Q&A. \n",
    "For the rest of the students, with a worse performance, one sees significantly more different connections between the courses, suggesting that courses are also skipped. For example, there are 28 students who started with Lecture 3 and almost 1000, and thus the majority, who started with Assignment 1 without having looked at any lectures before that. However, it is still evident that a majority of the students who have registered for the exam take all the events. So it seems that the order in which one attends the events has an influence on the result or when one starts to study, because the events are activated gradually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad464fd",
   "metadata": {},
   "source": [
    "## Performance and Frequency Decoration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6870ab5",
   "metadata": {},
   "source": [
    "**h)** While the discovery of a process model is the most prototypical step in a process mining analysis, its enrichment by frequency and performance statistics is a very common step too. To this end, enrich the Petri net that you discovered for log_exam using Inductive Miner with noise threshold 0.2 by frequency and performance information. Plot two Petri nets decorated with frequency and performance information, respectively.\n",
    "\n",
    "Describe your results. Can you observe any problems (in particular with respect to the initial process description)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55022af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "proc_tree = pm4py.discover_process_tree_inductive(log_exam, noise_threshold=0.2)\n",
    "net, im, fm = pt_converter.apply(proc_tree)\n",
    "parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=log_exam)\n",
    "dfg_visualization.view(gviz)\n",
    "gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.PERFORMANCE, log=log_exam)\n",
    "dfg_visualization.view(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f81244",
   "metadata": {},
   "source": [
    "Don't understand, why in these petri nets, there is no real order, in which the differnet events are taken, even though there are obviously overlaps.\n",
    "# comment: formulate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15db5a",
   "metadata": {},
   "source": [
    "## Process Mining Meets Advanced Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffada6a",
   "metadata": {},
   "source": [
    "### Studying Activity Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fa171",
   "metadata": {},
   "source": [
    "**a)** In this task, we are going to use advanced visualization techniques to create an overview over the course acitivities over time (**log.csv**).\n",
    "In particular, you shall create a heatmap that shows how often activities occur in a particular week. \n",
    "Your heatmap should adhere to the following specification:\n",
    "- y-axis: Shows the activity labels\n",
    "- x-axis: Time in terms of course weeks. See the following example snippet:\n",
    "<div>\n",
    "<img src=\"templates/PMAV_HeatmapXAxisSnippet.png\" width=\"100\"/>\n",
    "</div>\n",
    "- data: The bucket counts should be derived from **log.csv**\n",
    "\n",
    "Using this configuration, the bucket 2021-11-24 till 2021-12-01 with y-axis label \"Exam\" and value v would be read as:\n",
    "In the week between 2021-11-24 and 2021-12-01 v exams took place.\n",
    "\n",
    "Describe your result. Which patterns do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ac637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "log_grouped_week = df.copy(deep=True)\n",
    "log_grouped_week['time:timestamp'] = log_grouped_week['time:timestamp'].apply(lambda x: (x - timedelta(days=x.weekday())))\n",
    "log_grouped_week = log_grouped_week.astype({'time:timestamp': 'str'})\n",
    "log_grouped_week['time:timestamp'] = log_grouped_week['time:timestamp'].apply(lambda x: x[0:10])\n",
    "\n",
    "df_m = log_grouped_week.groupby([\"time:timestamp\", \"concept:name\"]).size().unstack(level=0)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(df_m, ax=ax)\n",
    "# ax.set_xticklabels(log_grouped_week['Timestamp'].dt.strftime('%d-%m-%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c63c4",
   "metadata": {},
   "source": [
    "Observed patterns:\n",
    "- Registration for the exam occurred over a relatively long period of time probably up to the end of the registration period during the week of 2021/11/29.\n",
    "- The submission phase of Assignment 1 and 2 was most likely in a 2-week time period\n",
    "- The 6 lectures were first consumed in a relatively evenly spaced period from the beginning of the semester to the end of the semester with a cut at the first Assignment. Maybe due to the preperation for the assignment, people only consumend the lectures relevant for the first assignment until it needed to be submitted. However, it is not possible to say whether individuals also consumed them consecutively, as there is still a large overlap between lectures.\n",
    "- 2 weeks before the exam and after the first assignment the lectures were repeated again by a small part of the students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae93fb",
   "metadata": {},
   "source": [
    "**b)** Can you relate the patterns that you observe in the heatmap to the process models that you discovered in question **Q5 - f)**?\n",
    "\n",
    "*Hint: In contrast to the other questions, this question is deliberately less explicit. You may approach it having the following question in mind: \\\n",
    "Is there a pattern in the heatmap that explains why a certain process model shows a certain behavior/control flow?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c0179",
   "metadata": {},
   "source": [
    "The heat map only shows which events took place at which point in time. However, in the case of overlaps, which can often be seen here, no statement can be made about what exactly the individual traces look like. Only an upper trend can be seen, which can be used to make assumptions about what the control flow will most likely look like. However, really explicit statements can only be made about how often individual events have taken place and at what time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afb9b5",
   "metadata": {},
   "source": [
    "### Process Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bb53a",
   "metadata": {},
   "source": [
    "**c)** Assume that another process analyst also had access to the event log. \n",
    "Given the data, he created the following novel entities:\n",
    "- 'Block 1 Complete': The participant downloaded the entire material of the first lecture block (materials 1, 2, and 3). (Not considering when he downloaded it)\n",
    "- 'Block 1 Incomplete': The participant did not download the entire material of the first lecture block\n",
    "- 'Block 2 Complete': See 'Block 1 Complete'\n",
    "- 'Block 2 Incomplete': See 'Block 1 Incomplete'\n",
    "- 'Ass 1 Excellent': Participant scored at least 85 points in the first part of the assignment\n",
    "- 'Ass 1 Not Excellent': Participant scored less than 85 points in the part of the assignment\n",
    "- 'Ass 2 Not Excellent', 'Ass 2 Excellent', 'Exam Not Excellent', 'Exam Excellent': Similar to 'Ass 1 Excellent' and 'Ass 1 Not Excellent'\n",
    "- 'Withdraw': Participant dropped the course\n",
    "\n",
    "Based on the entities he derived a set of flow, for example, the flow between 'Block 1 Complete' and 'Ass 1 Excellent' describes how often a partipant who consumed the first lecture block scored excellent in the first part of the assignment.\n",
    "\n",
    "The next cell loads the entities and flows for you. The flows are stored as a dictionary following the pattern:\n",
    "\n",
    "    (source, target): flow_value\n",
    "    \n",
    "where source and target are indices into the entities list.\n",
    "\n",
    "Create a **Sankey diagram** that visulizes these flows. Please use `plotly.graph_object.Sankey` to create the diagram.\n",
    "\n",
    "Briefly **describe** your results.\n",
    "Moreover, **discuss** this visualization considering your knowlege from the Process Mining task.\n",
    "\n",
    "*Hint: Having a look at the Sankey diagram will make the analyst's idea behind the entities much clearer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/sankeyEntities.pkl', 'rb') as f:\n",
    "    entities = pickle.load(f)\n",
    "with open('./dataset/sankeyFlows.pkl', 'rb') as f:\n",
    "    flows = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9862d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "l_source = []\n",
    "l_target = []\n",
    "l_values = []\n",
    "for key in flows:\n",
    "    l_source.append(key[0])\n",
    "    l_target.append(key[1])\n",
    "    l_values.append(flows[key])\n",
    "\n",
    "# Define Sankes diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = entities,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = l_source,\n",
    "      target = l_target,\n",
    "      value = l_values\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809f33c",
   "metadata": {},
   "source": [
    "As analyzed before, it can be clearly seen that the students who completed both lecture blocks and passed the assignments excellently, also passed the exam with excellent results for the most part. In addition, a large part of the students who withdraw from the course, did so after the first block or after a not so successful first assignment. The number of drop-outs decreases the later the course is. Except after the second block, every student continued to the second assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fb898",
   "metadata": {},
   "source": [
    "# Question 6 - Big Data (15 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa3c80a",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "You are working at a finance company that makes loans to individuals and businesses. As a process analyst in *business intelligence team*, you are expected to deliver data-driven insights to improve business processes of the company. Recently, your boss asked you to discover a comprehensive process model of 10 international branches using your big data skills. Your colleague already tried it using commercial on-premise tools, but, due to the immense size of the data, he didn't manage to even load the data to the tool. You are planning to 1) load the datasets from 10 different branches to Hadoop Distributed File System (HDFS), 2) preprocess them using HDFS, and 3) use MapReduce programming model to discover a comprehensive process model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051c3ab",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "The preparation of this problem consists of two steps:\n",
    "\n",
    "**Preparation step 1**: Replace the filepath to your own filepath to produce the **LoanApplication.csv**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74fbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#your filepath\n",
    "filepath = \"./dataset/LoanApplication.csv\"\n",
    "original_log = pd.read_csv(filepath,sep=\",\")\n",
    "original_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca27bd3",
   "metadata": {},
   "source": [
    "**Preparation step 2**: In this question, we generate 10 event logs based on the ``original_log``. For randomization, you need to use the sum of the group's matriculation numbers (e.g., a group with 3 students having \"100000\", \"100001\", and \"100002\" as their matriculation numbers will use \"300003\" for the randomization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfa2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are GIVEN utility functions (do not modify):\n",
    "import random\n",
    "import os\n",
    "def _ramdomize(x):\n",
    "    random_val = random.randint(5,10)\n",
    "    return x+random_val\n",
    "\n",
    "def _randomize_log(log,matriculation_num):\n",
    "    \"\"\"Randomize case attributes based on the matriculation number\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    matriculation_num - sum of matriculation numbers\n",
    "    \"\"\"\n",
    "    attribute_cols = [\"Duration\"]\n",
    "    random.seed(matriculation_num)\n",
    "    for attr in attribute_cols:\n",
    "        log[attr] = log[attr].apply(_ramdomize)\n",
    "    return log\n",
    "\n",
    "def _extract_log(log,iter_num):\n",
    "    \"\"\"Extract n-th log to ./generated_logs/\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    iter_num -- n-th iteration\n",
    "    \"\"\"\n",
    "    log.to_csv(\"./generated_logs/generated_log-{}.tsv\".format(iter_num),header=False,index=False, sep=\"\\t\",line_terminator=\"\")\n",
    "\n",
    "def generate_log(original_log,num_replication,mat_num):\n",
    "    \"\"\"Generate logs (randomized by the matriculation number and extracted to ./generated_logs/) \n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    num_replication -- number of generated logs\n",
    "    mat_num -- sum of matriculation numbers\n",
    "    \"\"\"\n",
    "    case_col=\"CaseID\"\n",
    "    timestamp_col = \"Timestamp\"\n",
    "    dir_path = \"./generated_logs\"\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except OSError:\n",
    "        print (\"Directory already exists: %s\" % dir_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % dir_path)\n",
    "    \n",
    "    for i in range(num_replication):\n",
    "        print(\"starts {}\".format(i))\n",
    "        generated_log = original_log.copy(deep=True)\n",
    "        generated_log[case_col] = str(i) + generated_log[case_col]\n",
    "        generated_log[timestamp_col] = generated_log[timestamp_col].apply(str).str.zfill(10)\n",
    "        randomized_log = _randomize_log(generated_log,mat_num)\n",
    "        _extract_log(randomized_log,i)\n",
    "        print (\"Successfully created %i th log at %s \"% (i,dir_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc8528",
   "metadata": {},
   "source": [
    "Replace the SUM_MAT_NUM to yours to generate logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "SUM_MAT_NUM = 154031 \n",
    "NUM_REPITITION=10\n",
    "generate_log(original_log,NUM_REPITITION,SUM_MAT_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28111a",
   "metadata": {},
   "source": [
    "### 6.1. Hadoop Distributed File System (HDFS)\n",
    "\n",
    "Now, it's time to work with the Hadoop Distributed File System (HDFS). The goal of this task is to merge 10 event logs at your disk using HDFS. Follow the instructions below and show your results in each step (screenshots of the command line). We use \"letter identifier\" for this task (The letter identifier is the string consisting of the first letters of the group memebers' first names, e.g., for the group with \"Antonio Rüdiger\", \"Bernd Leno\", \"Christian Günter\", the indentifier is \"ABC\").\n",
    "\n",
    "    1) Import the event logs to your Docker engine (at /usr/local/hadoop/(identifier)-generated-logs/).\n",
    "    2) Upload the event logs to the running HDFS (at /input/(identifier)-generated-logs/). \n",
    "    3) Merge all the files and copy the result back to HDFS (at /input/(identifier)-final-log-10.tsv).\n",
    "    4) Merge 6 files (you can randomly select) and copy the result back to HDFS (at /input/(identifier)-final-log-6.tsv).\n",
    "    5) Merge 2 files (you can randomly select) and copy the result back to HDFS (at /input/(identifier)-final-log-2.tsv).\n",
    "    6) Print out the completely-merged event log from 3), i.e., \"(identifier)-final-log-10.tsv\", in the command line (the screenshot may contain 10 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c14280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "from IPython.display import Image\n",
    "# Image(filename='your_path_to_screenshot_of_a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#Image(filename='your_path_to_screenshot_of_a2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3849bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#Image(filename='your_path_to_screenshot_of_a3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#Image(filename='your_path_to_screenshot_of_a4') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1f95b",
   "metadata": {},
   "source": [
    "### 6.2. Process Discovery\n",
    "\n",
    "Discover a process model from the completely merged event log using MapReduce algorithms. Explain how you discover the process model with the following deliverables:\n",
    "\n",
    "    1) Mapper function (as python file(s))\n",
    "    2) Reducer function (as python file(s))\n",
    "    3) Hadoop commands for MapReduce computation (as text file)\n",
    "    4) Jupyter notebook script that visualize (1) a directly-follows graph and (2) a Petri net  based on the computed directly-follows relations.\n",
    "\n",
    "<font color='red'>Important!</font> Please note that in this task, your result will be evaluated based on whether they are reproducible from your explanation. If you skip MapReduce computations for this task, you will get 0 points.The deliverables of 1), 2), and 3) should be submitted as outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245c0c4",
   "metadata": {},
   "source": [
    "### 6.3. Performance Analysis\n",
    "\n",
    "a) Compute the total service time for each case based on MapReduce algorithms using the completely-merged event log (i.e., (identifier)-final-log-10.tsv) and visualize 100 cases that show the longest total service time using any chart.\n",
    "    \n",
    "The deliverables of 1), 2), 3) and 4) should be submitted as outputs:\n",
    "```\n",
    "1) Mapper function (as python file(s))\n",
    "2) Reducer function (as python file(s))\n",
    "3) Hadoop commands for MapReduce calculation (as text file)\n",
    "4) Result: total service times for cases (as text file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbf91d",
   "metadata": {},
   "source": [
    "b) Compare the (approximate) computation time of the service time calculation between 1. the completely-merged event log (i.e., (identifier)-final-log-10.tsv), 2. 6-merged event log (i.e., (identifier)-final-log-6.tsv), and 3. 2-merged event log (i.e., (identifier)-final-log-2.tsv). Interpret the difference (e.g., the computation time scales linearly with the increasing number of events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
